from pathlib import Path
import os
import sys
import yaml
import re
import pandas as pd
from peppy import Project
from snakemake.utils import validate, min_version

##### set minimum snakemake version #####
min_version("8.20.1")

module_name = "hic_pipeline"

##### load config and sample annotation sheets #####
configfile: os.path.join("config", "test_config.yml")

# Load PEP project to get sample metadata
pep_config_value = config.get("pep_config", os.path.join("pep", "project_config.yaml"))
# Resolve PEP path; if relative, treat it as relative to the repo root (one level above workflow/)
if os.path.isabs(pep_config_value):
    pep_config_path = pep_config_value
else:
    pep_config_path = os.path.abspath(os.path.join(workflow.basedir, "..", pep_config_value))

if not os.path.exists(pep_config_path):
    raise OSError(f"Project config file path does not exist: {pep_config_path}")

# Load PEP project
pep_project = Project(pep_config_path)

# Merge PEP path variables into config for easier access
if "paths" in pep_project.config:
    if "paths" not in config:
        config["paths"] = {}
    config["paths"].update(pep_project.config["paths"])

# Helper function to resolve path placeholders in config
def resolve_config_paths(cfg, paths_dict):
    """Recursively resolve path placeholders in config dictionary"""
    if isinstance(cfg, dict):
        return {k: resolve_config_paths(v, paths_dict) for k, v in cfg.items()}
    elif isinstance(cfg, list):
        return [resolve_config_paths(item, paths_dict) for item in cfg]
    elif isinstance(cfg, str) and "{" in cfg and "}" in cfg:
        for key, value in paths_dict.items():
            cfg = cfg.replace(f"{{{key}}}", value)
        return cfg
    else:
        return cfg

# Resolve path placeholders in config
# First, resolve PEP paths (for placeholders like {process_dir}, {database_dir})
paths_dict = {}
if "paths" in config:
    paths_dict.update(config["paths"])
    config = resolve_config_paths(config, paths_dict)

# Then resolve output_base_dir itself if it contains placeholders
if "output_base_dir" in config and isinstance(config["output_base_dir"], str) and "{" in config["output_base_dir"]:
    config["output_base_dir"] = resolve_config_paths(config["output_base_dir"], paths_dict)

# Finally, add output_base_dir to paths_dict and resolve {output_base_dir} placeholders in output paths
if "output_base_dir" in config:
    paths_dict["output_base_dir"] = config["output_base_dir"]
    config = resolve_config_paths(config, paths_dict)

# Get sample table from PEP
annot = pep_project.sample_table.copy()

# Explode list-valued columns if any
list_cols = [col for col in annot.columns if annot[col].apply(lambda x: isinstance(x, list)).any()]
if list_cols:
    for col in list_cols:
        annot = annot.explode(col, ignore_index=True)

# Remove duplicate (sample_name, run) pairs
if 'sample_name' in annot.columns and 'run' in annot.columns:
    annot = annot.drop_duplicates(subset=['sample_name', 'run'], keep='first').reset_index(drop=True)

# Ensure required columns exist
required_columns = {"sample_name", "run"}
missing_required = required_columns.difference(annot.columns)
if missing_required:
    raise ValueError(f"Sample table missing required columns: {', '.join(sorted(missing_required))}.")

# Ensure run column is integer for schema validation
annot['run'] = pd.to_numeric(annot['run'], errors='raise').astype(int)

# Ensure passqc column is integer if it exists
if 'passqc' in annot.columns:
    annot['passqc'] = pd.to_numeric(annot['passqc'], errors='coerce').fillna(0).astype(int)

# Normalize skip_ligation to boolean if present
if 'skip_ligation' in annot.columns:
    annot['skip_ligation'] = (
        annot['skip_ligation']
        .astype(str)
        .str.strip()
        .str.lower()
        .map({'true': True, 'false': False})
    )
    # Default to False for any unrecognized/NA values
    annot['skip_ligation'] = annot['skip_ligation'].fillna(False).astype(bool)

# Create a unique identifier for each run: sample_name_run (store run as string for downstream paths)
annot['sample_name'] = annot['sample_name'].astype(str)
annot['sample_run'] = annot['sample_name'] + '_' + annot['run'].astype(str)

# Validate sample table
validate(annot, schema="../schemas/sample.schemas.yml")

# Convert run back to string for downstream path building
annot['run'] = annot['run'].astype(str)

# Check for duplicate sample_run identifiers
duplicates = annot[annot.duplicated(subset=['sample_run'], keep=False)]
if not duplicates.empty:
    sys.stderr.write("\nError: Duplicate (sample_name + run) pairs found in sample sheet!\n")
    sys.stderr.write("The following rows are not unique:\n")
    sys.stderr.write(str(duplicates[['sample_name', 'run', 'sample_run']]) + "\n\n")
    sys.exit(1)

# Set sample_run as index
annot = annot.set_index('sample_run')

# Get unique sample names (for downstream analysis)
samples = annot.reset_index().drop_duplicates(subset='sample_name', keep='first').set_index("sample_name").to_dict(orient="index")

# Include common functions
include: "rules/common.smk"

localrules:
    merge_stats_libraries_into_groups,

# Set up output directories
result_path = config.get("output_base_dir", "results")
processed_fastqs_folder = config["output"]["dirs"]["processed_fastqs"]
mapped_parsed_sorted_chunks_folder = config["output"]["dirs"]["mapped_parsed_sorted_chunks"]
pairs_runs_folder = config["output"]["dirs"]["pairs_runs"]
pairs_library_folder = config["output"]["dirs"]["pairs_library"]
coolers_library_folder = config["output"]["dirs"]["coolers_library"]
coolers_library_group_folder = config["output"]["dirs"]["coolers_library_group"]
stats_library_group_folder = config["output"]["dirs"]["stats_library_group"]
downstream_dist_folder = config["output"]["dirs"]["downstream_dist_folder"]
downstream_loops_folder = config["output"]["dirs"]["downstream_loops_folder"]
multiqc_folder = config["output"]["dirs"]["multiqc"]

# Genome configuration
assembly = config["input"]["genome"]["assembly_name"]
genome_path = config["input"]["genome"]["bwa_index_wildcard_path"].rstrip("*")
bowtie_index_path = config["input"]["genome"]["bowtie_index_path"].rstrip("*")
chromsizes_path = config["input"]["genome"]["chrom_sizes_path"]

# Build LIBRARY_RUN_FASTQS from annot
LIBRARY_RUN_FASTQS = {}
SAMPLE_METADATA = {}

for sample_run_id, row in annot.iterrows():
    library = row['sample_name']
    run = row['run']
    
    # Get fastq paths (support both R1/R2 and fastq1/fastq2)
    if 'R1' in row and 'R2' in row:
        fastq1, fastq2 = row['R1'], row['R2']
    elif 'fastq1' in row and 'fastq2' in row:
        fastq1, fastq2 = row['fastq1'], row['fastq2']
    else:
        raise ValueError(f"Sample {sample_run_id} missing R1/R2 or fastq1/fastq2 columns")
    
    if library not in LIBRARY_RUN_FASTQS:
        LIBRARY_RUN_FASTQS[library] = {}
        SAMPLE_METADATA[library] = {}
    
    LIBRARY_RUN_FASTQS[library][run] = [str(fastq1), str(fastq2)]
    
    # Store metadata
    SAMPLE_METADATA[library][run] = {}
    if 'ligation_site' in row and pd.notna(row['ligation_site']):
        SAMPLE_METADATA[library][run]['ligation_site'] = str(row['ligation_site'])
    if 'skip_ligation' in row and pd.notna(row['skip_ligation']):
        SAMPLE_METADATA[library][run]['skip_ligation'] = bool(row['skip_ligation'])

# Automatically create library_groups if all_group is True
if config["input"].get("all_group", False):
    all_libraries = list(LIBRARY_RUN_FASTQS.keys())
    if "library_groups" not in config["input"]:
        config["input"]["library_groups"] = {}
    config["input"]["library_groups"]["all"] = all_libraries
    print(f"Auto-generated 'all' group with {len(all_libraries)} libraries: {all_libraries}")

runs = [list(LIBRARY_RUN_FASTQS[lib].keys()) for lib in LIBRARY_RUN_FASTQS.keys()]
runs = [item for sublist in runs for item in sublist]

# No chunking - treat each run as a single chunk named "raw"
CHUNK_IDS = {
    library: {run: ["raw"] for run in LIBRARY_RUN_FASTQS[library]}
    for library in LIBRARY_RUN_FASTQS
}
all_chunk_ids = ["raw"]

def get_raw_fastqs(wc):
    """Return raw R1/R2 FASTQ paths for a given library/run."""
    fastqs = LIBRARY_RUN_FASTQS[wc.library][wc.run]
    return fastqs

min_resolution = min(config["bin"]["resolutions"])

# Setting up scaling from pairs outputs if required
library_scaling_pairs = []
if "scaling_pairs" in config and config["scaling_pairs"].get("do", True):
    library_scaling_pairs = expand(
        f"{pairs_library_folder}/{{library}}.{assembly}.nodups.scaling.tsv",
        library=LIBRARY_RUN_FASTQS.keys(),
    )
    if not config["scaling_pairs"].get("max_distance", False):
        chromsizes = pd.read_table(
            chromsizes_path, header=None, names=["chrom", "size"]
        )
        config["scaling_pairs"]["max_distance"] = chromsizes["size"].max()

# Setting up the library cooler outputs
library_coolers = expand(
    f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{min_resolution}.mcool",
    library=LIBRARY_RUN_FASTQS.keys(),
    filter_name=list(config["bin"]["filters"].keys()),
)

library_downstream_stat = expand(
    f"{downstream_dist_folder}/{{library}}.{{filter_name}}.{min_resolution}_loglog_fits.csv",
    library=LIBRARY_RUN_FASTQS.keys(),
    filter_name=list(config["bin"]["filters"].keys()),
)

library_mustache_loops = expand(
    f"{downstream_loops_folder}/{{library}}.{{filter_name}}.{{resolution}}_loops.tsv",
    library=LIBRARY_RUN_FASTQS.keys(),
    filter_name=list(config["bin"]["filters"].keys()),
    resolution=config["bin"]["resolution_for_mustache"],
)

library_hic = []
if config["bin"].get("make_hic", False):
    library_hic += expand(
        f"{coolers_library_folder}/{{library}}.{assembly}.{{filter_name}}.{min_resolution}.hic",
        library=LIBRARY_RUN_FASTQS.keys(),
        filter_name=list(config["bin"]["filters"].keys()),
    )

# Setting up the library group cooler and group stats outputs
library_group_coolers = []
library_group_hic = []
library_group_stats = []
if "library_groups" in config["input"] and len(config["input"]["library_groups"]) > 0:
    library_group_coolers = expand(
        f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{min_resolution}.mcool",
        library_group=config["input"]["library_groups"].keys(),
        filter_name=list(config["bin"]["filters"].keys()),
    )
    if config["bin"].get("make_hic", False):
        library_group_hic = expand(
            f"{coolers_library_group_folder}/{{library_group}}.{assembly}.{{filter_name}}.{min_resolution}.hic",
            library_group=config["input"]["library_groups"].keys(),
            filter_name=list(config["bin"]["filters"].keys()),
        )
    library_group_stats = expand(
        f"{stats_library_group_folder}/{{library_group}}.{assembly}.stats",
        library_group=config["input"]["library_groups"].keys(),
    )

assemble_mapstats = []
assemble_pairstats = []
for library in LIBRARY_RUN_FASTQS:
    assemble_mapstats.append(
        f"{mapped_parsed_sorted_chunks_folder}/{library}/assemble.mapstat"
    )
    assemble_pairstats.append(
        f"{mapped_parsed_sorted_chunks_folder}/{library}/assemble.pairstat"
    )

multiqc = [f"{multiqc_folder}/multiqc_report.html"]
combined_dedup_stats = f"{pairs_library_folder}/combined_dedup_stats_summary.tsv"
combined_mapstats = f"{pairs_library_folder}/combined_mapstats_summary.tsv"
combined_pairstats = f"{pairs_library_folder}/combined_pairstats_summary.tsv"
combined_RSstats = f"{pairs_library_folder}/combined_RSstats_summary.tsv"
combined_hicproPairstats = f"{pairs_library_folder}/combined_hicpro_pairstats_summary.tsv"
combined_stats = [combined_dedup_stats, combined_mapstats, combined_pairstats, combined_RSstats, combined_hicproPairstats]

allvalidPair = []
for library in LIBRARY_RUN_FASTQS:
    allvalidPair.append(f"{pairs_library_folder}/{library}.allValidPairs")

# Some sanity checks - Phasing
if config.get('phase', {}).get('do_phase', False):
    if config['map']['mapper'] == 'chromap':
        raise ValueError(
            "Phasing is not possible with chromap, please use bwa-mem, bwa-mem2 or bwa-meme."
        )
    parse_options = argstring_to_dict(config['parse'].get('parsing_options', ''))
    if '--min-mapq' not in parse_options or parse_options['--min-mapq'] != '0':
        raise ValueError(
            "Please set '--min-mapq to 0' in the parsing options to use phasing."
        )
    if '--add-columns' not in parse_options:
        raise ValueError(
            "Please set the appropriate --add-columns argument in the parsing options to use phasing."
        )
    elif config['phase']['tag_mode'] == 'XA' and not set(['XA', 'NM', 'AS', 'XS', 'mapq']).issubset(set(parse_options['--add-columns'].split(','))):
        raise ValueError(
            "Please set '--add-columns XA,NM,AS,XS,mapq' in the parsing options to use phasing with XA tag mode."
        )
    elif config['phase']['tag_mode'] == 'XB' and not set(['XB', 'NM', 'AS', 'XS', 'mapq']).issubset(set(parse_options['--add-columns'].split(','))):
        raise ValueError(
            "Please set '--add-columns XB,NM,AS,XS,mapq' in the parsing options to use phasing with XB tag mode."
        )
    dedup_options = config["dedup"].get("dedup_options", "") 
    if '--extra-col-pair phase1 phase1' not in dedup_options or '--extra-col-pair phase2 phase2' not in dedup_options:
        raise ValueError(
            "Please add '--extra-col-pair phase1 phase2' in the dedup options to use phasing."
        )

# List of all required outputs of the workflow
rule default:
    input:
        library_group_coolers,
        library_group_hic,
        library_group_stats,
        library_coolers,
        library_hic,
        library_scaling_pairs,
        multiqc,
        combined_stats,
        assemble_mapstats,
        allvalidPair,
        library_downstream_stat,
        library_mustache_loops

# Global constraints on the wildcards
wildcard_constraints:
    library=f"({'|'.join([re.escape(lib) for lib in LIBRARY_RUN_FASTQS.keys()])})",
    library_group=(
        f"({'|'.join([re.escape(lib) for lib in config['input']['library_groups'].keys()])})"
        if "library_groups" in config["input"]
        else ""
    ),
    run=f"({'|'.join([re.escape(run) for run in runs])})",
    chunk_id=f"({'|'.join(all_chunk_ids)})",
    resolution=f"({'|'.join([str(res) for res in config['bin']['resolutions']])})",

# Depending on the mapper, the index files will be different
if config["map"]["mapper"] == "bwa-mem":
    idx = multiext(
        genome_path,
        ".amb",
        ".ann",
        ".bwt",
        ".pac",
        ".sa",
    )
elif config["map"]["mapper"] == "bwa-mem2":
    idx = multiext(
        genome_path,
        ".0123",
        ".amb",
        ".ann",
        ".bwt.2bit.64",
        ".pac",
    )
elif config["map"]["mapper"] == "bowtie2":
    idx = multiext(
        bowtie_index_path,
        ".1.bt2",
        ".2.bt2",
        ".3.bt2",
        ".4.bt2",
        ".rev.1.bt2",
        ".rev.2.bt2",
    )
else:  # bwa-meme
    idx = multiext(
        genome_path,
        ".0123",
        ".amb",
        ".ann",
        ".pac",
        ".pos_packed",
        ".suffixarray_uint64",
        ".suffixarray_uint64_L0_PARAMETERS",
        ".suffixarray_uint64_L1_PARAMETERS",
        ".suffixarray_uint64_L2_PARAMETERS",
    )

if config["map"]["mapper"] == "chromap":
    ruleorder: map_chunks_chromap > parse_sort_chunks
elif config["map"]["mapper"] == "bowtie2":
    pass
else:
    ruleorder: parse_sort_chunks > map_chunks_chromap

include: "rules/pairtools.smk"
include: "rules/report.smk"
include: "rules/preprocess.smk"
include: "rules/align.smk"
include: "rules/hicpro_style.smk"
include: "rules/cooltools.smk"
include: "rules/downstream.smk"

# If we keep bams, simply store the output in the folder, otherwise pipe it to pairtools parse
if config["map"]["mapper"] in ["bwa-mem", "bwa-mem2", "bwa-meme"]:
    if config["parse"]["keep_unparsed_bams"]:
        ruleorder: map_chunks_bwa > map_chunks_bwa_pipe
    else:
        ruleorder: map_chunks_bwa_pipe > map_chunks_bwa

    use rule map_chunks_bwa as map_chunks_bwa_pipe with:
        output:
            pipe(
                f"{mapped_parsed_sorted_chunks_folder}/{{library}}/{{run}}/{{chunk_id}}.bam"
            ),

rule merge_stats_libraries_into_groups:
    input:
        lambda wildcards: expand(
            f"{pairs_library_folder}/{{library}}.{assembly}.dedup.stats",
            library=config["input"]["library_groups"][wildcards.library_group],
        ),
    conda:
        "envs/pairtools_cooler.yml"
    log:
        "logs/merge_stats_libraries_into_groups/{library_group}.log",
    output:
        f"{stats_library_group_folder}/{{library_group}}.{assembly}.stats",
    shell:
        r"pairtools stats --merge {input} -o {output} >{log[0]} 2>&1"
